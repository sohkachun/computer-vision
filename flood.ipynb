{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n       # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-10T03:04:50.948200Z","iopub.execute_input":"2023-11-10T03:04:50.948568Z","iopub.status.idle":"2023-11-10T03:04:50.954412Z","shell.execute_reply.started":"2023-11-10T03:04:50.948539Z","shell.execute_reply":"2023-11-10T03:04:50.953290Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from torch_snippets import *\nfrom torch_snippets.inspector import inspect\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n#!pip install utils\nimport utils\nfrom torchvision import transforms as T\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:04:50.956838Z","iopub.execute_input":"2023-11-10T03:04:50.957297Z","iopub.status.idle":"2023-11-10T03:04:52.227265Z","shell.execute_reply.started":"2023-11-10T03:04:50.957264Z","shell.execute_reply":"2023-11-10T03:04:52.225492Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement transfoms (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transfoms\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"all_images = Glob('/kaggle/input/flood-area-segmentation/Image')\nall_annots = Glob('/kaggle/input/flood-area-segmentation/Mask')","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:04:52.228972Z","iopub.execute_input":"2023-11-10T03:04:52.229288Z","iopub.status.idle":"2023-11-10T03:04:52.250090Z","shell.execute_reply.started":"2023-11-10T03:04:52.229255Z","shell.execute_reply":"2023-11-10T03:04:52.248339Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"annots = []\nfor ann in Tqdm(all_annots[:5000]):\n    _ann = read(ann, 1).transpose(2,0,1)\n    r,g,b = _ann\n    if 4 not in np.unique(r): continue\n    annots.append(ann)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:04:52.252881Z","iopub.execute_input":"2023-11-10T03:04:52.253199Z","iopub.status.idle":"2023-11-10T03:05:01.494334Z","shell.execute_reply.started":"2023-11-10T03:04:52.253171Z","shell.execute_reply":"2023-11-10T03:05:01.492971Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 290/290 [00:09<00:00, 31.42it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n_annots = stems(annots)\ntrn_items, val_items = train_test_split(_annots, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:05:01.495534Z","iopub.execute_input":"2023-11-10T03:05:01.495958Z","iopub.status.idle":"2023-11-10T03:05:01.505203Z","shell.execute_reply.started":"2023-11-10T03:05:01.495930Z","shell.execute_reply":"2023-11-10T03:05:01.503274Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def get_transform(train):\n    image_transforms = []\n    image_transforms.append(T.PILToTensor())\n    if train:\n        image_transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(image_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:05:01.507024Z","iopub.execute_input":"2023-11-10T03:05:01.507376Z","iopub.status.idle":"2023-11-10T03:05:01.519514Z","shell.execute_reply.started":"2023-11-10T03:05:01.507346Z","shell.execute_reply":"2023-11-10T03:05:01.517836Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class MasksDataset(Dataset):\n    def __init__(self, items, transforms, N):\n        self.items = items\n        self.transforms = transforms\n        self.N = N\n    def get_mask(self, path):\n        an = read(path, 1).transpose(2,0,1)\n        r,g,b = an\n        nzs = np.nonzero(r==2)\n        instances = np.unique(g[nzs])\n        masks = np.zeros((len(instances), *r.shape))\n        for ix,_id in enumerate(instances):\n            masks[ix] = g==_id\n        return masks\n    def __getitem__(self, ix):\n        _id = self.items[ix]\n        img_path = f'/kaggle/input/flood-area-segmentation/Image/{_id}.jpg'\n        mask_path = f'/kaggle/input/flood-area-segmentation/Mask/{_id}.png'\n        masks = self.get_mask(mask_path)\n        obj_ids = np.arange(1, len(masks)+1)\n        img = Image.open(img_path).convert(\"RGB\")\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            obj_pixels = np.where(masks[i])\n            xmin = np.min(obj_pixels[1])\n            xmax = np.max(obj_pixels[1])\n            ymin = np.min(obj_pixels[0])\n            ymax = np.max(obj_pixels[0])\n            if (((xmax-xmin)<=10) | (ymax-ymin)<=10):\n                xmax = xmin+10\n                ymax = ymin+10\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n        image_id = torch.tensor([ix])\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :\n          img = img/255.\n        return img, target\n    def __len__(self):\n        return self.N\n    def choose(self):\n        return self[randint(len(self))]","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:05:55.852161Z","iopub.execute_input":"2023-11-10T03:05:55.852537Z","iopub.status.idle":"2023-11-10T03:05:55.867925Z","shell.execute_reply.started":"2023-11-10T03:05:55.852507Z","shell.execute_reply":"2023-11-10T03:05:55.866395Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"x = MasksDataset(trn_items, get_transform(train=True), N=100)\nim,targ = x[0]\ninspect(im,targ)\nsubplots([im, *targ['masks']], sz=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T03:12:16.323485Z","iopub.execute_input":"2023-11-10T03:12:16.323849Z","iopub.status.idle":"2023-11-10T03:12:16.407449Z","shell.execute_reply.started":"2023-11-10T03:12:16.323821Z","shell.execute_reply":"2023-11-10T03:12:16.406123Z"},"trusted":true},"execution_count":38,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m MasksDataset(trn_items, get_transform(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m im,targ \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m inspect(im,targ)\n\u001b[1;32m      4\u001b[0m subplots([im, \u001b[38;5;241m*\u001b[39mtarg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m]], sz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n","Cell \u001b[0;32mIn[34], line 48\u001b[0m, in \u001b[0;36mMasksDataset.__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     46\u001b[0m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iscrowd\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;129;01mor\u001b[39;00m (img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8) :\n\u001b[1;32m     50\u001b[0m   img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m\n","\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"Compose.__call__() takes 2 positional arguments but 3 were given","output_type":"error"}]}]}